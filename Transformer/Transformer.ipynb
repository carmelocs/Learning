{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Reading note for \"Attention Is All You Need\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from Model import Transformer, get_pad_mask, get_subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some fake data\n",
    "torch.manual_seed(0)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN_SEQ = 100\n",
    "LEN_SRC = 100\n",
    "LEN_TGT = 100\n",
    "D_WORD_VEC = 512\n",
    "\n",
    "src_word = torch.rand(BATCH_SIZE, LEN_SRC).long()\n",
    "tgt_word = torch.rand(BATCH_SIZE, LEN_TGT).long()\n",
    "# pc = torch.rand(100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# number of encoder/decoder layers\n",
    "NUM_LAYER = 6\n",
    "\n",
    "# The dimensionality of input and output for EncoderDecoder model\n",
    "D_MODEL = 512\n",
    "\n",
    "# number of heads/parallel attention layers\n",
    "NUM_HEAD = 8\n",
    "\n",
    "# The dimensionality of qurey and key in each head\n",
    "D_K = D_MODEL // NUM_HEAD\n",
    "# print(d_k)\n",
    "\n",
    "# The dimensionality of value in each head (could be different from d_k)\n",
    "D_V = D_K\n",
    "\n",
    "# The dimensionality of inner-layer for Position-wise Feed-Forward Network(FFN)\n",
    "D_FF = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "enc_output: torch.Size([16, 100, 512])\n",
      "enc_src_mask: torch.Size([16, 100, 100])\n",
      "trans_src_word: torch.Size([16, 100])\n",
      "trans_src_mask: torch.Size([16, 100, 100])\n",
      "transf_enc_output: torch.Size([16, 100, 512])\n",
      "trans_dec_output: torch.Size([16, 100, 512])\n",
      "seq_pred: torch.Size([16, 100, 100])\n",
      "pred: torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(len_src_vocab=LEN_SRC,\n",
    "                 len_tgt_vocab=LEN_TGT,\n",
    "                 d_word_vec=D_WORD_VEC,\n",
    "                 d_model=D_MODEL,\n",
    "                 num_head=NUM_HEAD,\n",
    "                 num_layer=NUM_LAYER,\n",
    "                 d_k=D_K,\n",
    "                 d_v=D_V,\n",
    "                 d_ff=D_FF,\n",
    ")\n",
    "pred = transformer(src_word, tgt_word)\n",
    "print(f\"pred: {pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "src_emb: torch.Size([16, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "emb = nn.Embedding(LEN_SRC, D_WORD_VEC)\n",
    "src_emb = emb(src_word)\n",
    "print(f\"src_emb: {src_emb.shape}\")\n",
    "\n",
    "# # linear project input x into corresponding dimentionalities\n",
    "# Wq = nn.Linear(d_x, d_model)\n",
    "# Wk = nn.Linear(d_x, d_model)\n",
    "# Wv = nn.Linear(d_x, d_model)\n",
    "\n",
    "# query = Wq(x)\n",
    "# key = Wk(x)\n",
    "# value = Wv(x)\n",
    "\n",
    "# print(f\"query: {query.shape}, \\nkey: {key.shape}, \\nvalue: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "    \n",
    "        d_k = query.size(-1) # [batch, len, d_k]\n",
    "\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        scores = scores.softmax(-1)\n",
    "\n",
    "        if self.dropout is not None:\n",
    "            scores = self.dropout(scores)\n",
    "\n",
    "        att = torch.matmul(scores, value)\n",
    "\n",
    "        return att, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention = ScaledDotProductAttention(dropout=0.1)\n",
    "# att, scores = attention(src_emb, src_emb, src_emb, pad_mask)\n",
    "# print(f\"scores: {scores.shape} \\nattention: {att.shape}, \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, d_k, d_v, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        self.h = h\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.wq = nn.Linear(d_model, h * d_k)\n",
    "        self.wk = nn.Linear(d_model, h * d_k)\n",
    "        self.wv = nn.Linear(d_model, h * d_v)\n",
    "        self.fc = nn.Linear(h * d_v, d_model)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        residual = query # [batch, len_seq, d_model]\n",
    "\n",
    "        # print(f\"query: {query.shape}, \\nkey: {key.shape}, \\nvalue: {value.shape}\")\n",
    "\n",
    "        # linear projection and split d_model by heads\n",
    "        query = self.wq(query).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) # [batch, h, len_query, d_k]\n",
    "        key = self.wk(key).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) # [batch, h, len_key, d_k]\n",
    "        value = self.wv(value).view(batch_size, -1, self.h, self.d_v).transpose(1, 2) # [batch, h, len_value, d_v]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).repeat(1, self.h, 1, 1)\n",
    "\n",
    "        att, scores = self.attention(query, key, value, mask) # att: [batch, h, len_seq, d_v], scores: [batch, h, len_seq, len_seq]\n",
    "        # print(f\"scores: {scores.shape}, \\nattention: {att.shape}\")\n",
    "\n",
    "        # concat heads\n",
    "        att_cat = att.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_v) # [batch, len_seq, d_model]\n",
    "\n",
    "        # final linear projection\n",
    "        output = self.fc(att_cat) # [batch, len_seq, d_model]\n",
    "\n",
    "        # dropout\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # add residual and norm layer\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_head = MultiHeadAttention(NUM_HEAD, D_MODEL, D_K, D_V)\n",
    "# output, scores = multi_head(src_emb, src_emb, src_emb, pad_mask)\n",
    "# print(f\"output of multi_head_attention: {output.shape}\\nscores: {scores.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        # self.w1 = nn.Conv1d(d_model, d_ff, 1)\n",
    "        # self.w2 = nn.Conv1d(d_model, d_ff, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.w2(self.relu(self.w1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm(x + residual)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffn = PositionWiseFeedForward(D_MODEL, D_FF)\n",
    "# out_ffn = ffn(output)\n",
    "# print(f\"output of ffn: {out_ffn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, h, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_att = MultiHeadAttention(h, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionWiseFeedForward(d_model, d_ff, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, slf_att_mask=None):\n",
    "        enc_output, enc_slf_att = self.slf_att(enc_input, enc_input, enc_input, mask=slf_att_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        # print(f\"enc_layer_slf_mask: {slf_att_mask.shape}\\nenc_layer_output: {enc_output.shape}\")\n",
    "        return enc_output, enc_slf_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_layer = EncoderLayer(D_MODEL, D_FF, NUM_HEAD, D_K, D_V)\n",
    "# enc_output, enc_slf_att = enc_layer(src_emb)\n",
    "# print(f\"enc_output: {enc_output.shape}\\nenc_slf_att: {enc_slf_att.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, h, d_k, d_v, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.slf_att = MultiHeadAttention(h, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.enc_att = MultiHeadAttention(h, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionWiseFeedForward(d_model, d_ff, dropout=dropout)\n",
    "\n",
    "    def forward(self, dec_input, enc_output, slf_att_mask=None, dec_enc_att_mask=None):\n",
    "        dec_output, dec_slf_att = self.slf_att(dec_input, dec_input, dec_input, mask=slf_att_mask)\n",
    "        dec_output, dec_enc_att = self.enc_att(dec_output,enc_output, enc_output, mask=dec_enc_att_mask)\n",
    "        dec_output = self.pos_ffn(dec_output)\n",
    "        return dec_output, dec_slf_att, dec_enc_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec_layer = DecoderLayer(D_MODEL, D_FF, NUM_HEAD, D_K, D_V)\n",
    "# dec_output, dec_slf_att, dec_enc_att = dec_layer(src_emb, enc_output)\n",
    "# print(f\"dec_output: {dec_output.shape}\\ndec_slf_att: {dec_slf_att.shape}\\ndec_enc_att: {dec_enc_att.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(seq_q, seq_k):\n",
    "\t# seq_k和seq_q的形状都是[batch, len_seq]\n",
    "    len_q = seq_q.size(1)\n",
    "    # `PAD` is 0\n",
    "    pad_mask = seq_k.eq(0)\n",
    "    # print(pad_mask.shape)\n",
    "    pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1)  # shape [batch, len_seq, len_seq]\n",
    "    return pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask = get_pad_mask(src_word, src_word)\n",
    "# print(f\"pad_mask: {pad_mask.shape}\") # [batch, len, len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsequent_mask(len_q, len_k):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    return torch.triu(torch.ones(1, len_q, len_k), diagonal=1) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "subsequent_mask: torch.Size([1, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "subsequent_mask = get_subsequent_mask(LEN_SRC, LEN_SRC)\n",
    "print(f\"subsequent_mask: {subsequent_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.figure(figsize=(5,5))\n",
    "# plt.imshow(subsequent_mask[0,:20, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, len_src_vocab, d_word_vec, num_layer, d_model, h, d_k, d_v, d_ff, dropout=0.1):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.src_word_emb = nn.Embedding(len_src_vocab, d_word_vec)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_ff, h, d_k, d_v, dropout) for _ in range(num_layer)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src_word, src_mask=None):\n",
    "\n",
    "        enc_output = self.layer_norm(self.dropout(self.src_word_emb(src_word)))\n",
    "        # print(f\"enc_output: {enc_output.shape}\")\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, *_ = enc_layer(enc_output, src_mask)\n",
    "        print(f\"enc_output: {enc_output.shape}\\nenc_src_mask: {src_mask.shape}\")\n",
    "\n",
    "        return enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder(LEN_SRC, D_WORD_VEC, NUM_LAYER, D_MODEL, NUM_HEAD, D_K, D_V, D_FF)\n",
    "# enc_output = encoder(src_word, pad_mask)\n",
    "# print(f\"encoder output: {enc_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, len_tgt_vocab, d_word_vec, num_layer, d_model, h, d_k, d_v, d_ff, dropout=0.1):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_word_emb = nn.Embedding(len_tgt_vocab, d_word_vec)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_ff, h, d_k, d_v, dropout) for _ in range(num_layer)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tgt_word, enc_output, tgt_mask=None, src_mask=None):\n",
    "        dec_output = self.layer_norm(self.dropout(self.tgt_word_emb(tgt_word)))\n",
    "        # print(f\"tgt_word: {tgt_word.shape}\\nenc_output: {enc_output.shape}\\ntgt_mask: {tgt_mask.shape}\\nsrc_mask: {src_mask.shape}\")\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, *_ = dec_layer(dec_output, enc_output, slf_att_mask=tgt_mask, dec_enc_att_mask=src_mask)\n",
    "        \n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt_mask = get_pad_mask(src_word, tgt_word) & get_subsequent_mask(LEN_SRC, LEN_TGT)\n",
    "# print(f\"tgt_mask: {tgt_mask.shape}\")\n",
    "# src_mask = get_pad_mask(src_word, tgt_word)\n",
    "# print(f\"src_mask: {src_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder = Decoder(LEN_TGT, D_WORD_VEC, NUM_LAYER, D_MODEL, NUM_HEAD, D_K, D_V, D_FF)\n",
    "# dec_output = decoder(tgt_word, enc_output, tgt_mask, src_mask)\n",
    "# print(f\"dec_output: {dec_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, len_src_vocab, len_tgt_vocab, d_word_vec, d_model, d_ff, num_layer, h, d_k, d_v, dropout=0.1):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.len_src_vocab = len_src_vocab\n",
    "        self.len_tgt_vocab = len_tgt_vocab\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            len_src_vocab=len_src_vocab, \n",
    "            d_word_vec=d_word_vec, \n",
    "            num_layer=num_layer, \n",
    "            d_model=d_model, \n",
    "            h=h, d_k=d_k, d_v=d_v, d_ff=d_ff, \n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            len_tgt_vocab=len_tgt_vocab,\n",
    "            d_word_vec=d_word_vec,\n",
    "            num_layer=num_layer,\n",
    "            d_model=d_model,\n",
    "            h=h, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.tgt_word_prj = nn.Linear(d_model, len_tgt_vocab)\n",
    "\n",
    "        assert d_word_vec == d_model\n",
    "\n",
    "    def forward(self, src_word, tgt_word):\n",
    "\n",
    "        src_mask = get_pad_mask(src_word, src_word)\n",
    "        tgt_mask = get_pad_mask(tgt_word, tgt_word) & get_subsequent_mask(self.len_tgt_vocab, self.len_tgt_vocab)\n",
    "        # print(f\"trans_src_mask: {src_mask.shape}\\ntrans_tgt_mask: {tgt_mask.shape}\")\n",
    "\n",
    "        enc_output = self.encoder(src_word=src_word, src_mask=src_mask)\n",
    "        # print(f\"trans_src_word: {src_word.shape}\\ntrans_src_mask: {src_mask.shape}\\ntransf_enc_output: {enc_output.shape}\\n\")\n",
    "        dec_output = self.decoder(tgt_word=tgt_word, enc_output=enc_output, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "        print(f\"trans_src_word: {src_word.shape}\\ntrans_src_mask: {src_mask.shape}\\ntransf_enc_output: {enc_output.shape}\\ntrans_dec_output: {dec_output.shape}\")\n",
    "        seq_pred = self.tgt_word_prj(dec_output)\n",
    "        print(f\"seq_pred: {seq_pred.shape}\")\n",
    "\n",
    "        return F.log_softmax(seq_pred, dim=-1).max(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "enc_output: torch.Size([16, 100, 512])\n",
      "enc_src_mask: torch.Size([16, 100, 100])\n",
      "trans_src_word: torch.Size([16, 100])\n",
      "trans_src_mask: torch.Size([16, 100, 100])\n",
      "transf_enc_output: torch.Size([16, 100, 512])\n",
      "trans_dec_output: torch.Size([16, 100, 512])\n",
      "seq_pred: torch.Size([16, 100, 100])\n",
      "pred: torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    len_src_vocab=LEN_SRC, len_tgt_vocab=LEN_TGT, d_word_vec=D_WORD_VEC, d_model=D_MODEL, d_ff=D_FF, num_layer=NUM_LAYER, h=NUM_HEAD, d_k=D_K, d_v=D_V\n",
    ")\n",
    "pred = transformer(src_word, tgt_word)\n",
    "print(f\"pred: {pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}